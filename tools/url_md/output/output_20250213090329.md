# https://zenn.dev/erukiti/articles/2502-gemini-20-flash

# ページコンテンツ
URL: https://zenn.dev/erukiti/articles/2502-gemini-20-flash
ドメイン: zenn.dev
取得日時: 2025-02-13 09:03:29

## 本文

erukiti

gemini-2.0-flashが賢くてコスパがよすぎる件

2025/02/12に公開

LLM

Gemini

tech

この記事はLLMをAPIで使うこと前提の記事です。

AI エージェント開発ハッカソン参加記事：Gemini 2.0 Flash で技術文書分析ツール「Tascario」を作ってみたの、技術的補足記事です。

APIでLLMを使っている皆さん gemini-2.0-flash を使っていますか？APIで使う限り、かなり性能が高く、コスパも良すぎて、APIならこれ一択で良いのでは？とすら思い始めています。

なお、flash-liteやflash-thinkingはまだAPIで試していないため評価ができていませんが、そちらも選択肢に入るかもしれません。flash-liteがどれくらいの性能かは気になります。コストがそこまで違うわけじゃないので悩ましいところですが。

この記事ではTypeScriptで書いていますが、Pythonなど他の言語でもできるので、お好きな対話型AIに聞いてみるといいでしょう。例えば、次のようなプロンプトです。

添付の記事はTypeScriptでサンプルが書かれているが、サンプル内容を全部Pythonに書き直して。それ以外はそのままで

この記事で得られる知見

gemini-2.0-flashの力がわかる

文章を分析して構造化出力を得られる

シンプルなプロンプトだと、アホの子になってしまう現象を回避して、gemini-2.0-flashの本領を発揮させる方法

gemini-2.0-flashの衝撃

もちろんタスクによって違うとは思いますが、筆者がLLMをAPIで使う時に重視するのは、与えたコンテキストの範囲で理解力が低下しないこと（100kトークン与えたら、100kトークン全部理解しててほしい）、指示に追従する能力と、日本語の読み書き性能です。

コンテキストの範囲内で取りこぼさない

指示に追従する

日本語の読み書き

その点でずっと優秀であり続けたのがClaude 3.5 Sonnetです。コイツはこの3つの能力が尋常ではなく高いです。でも、お値段も高く、レスポンスも遅いです。大量のデータ処理をやらせるには、コスパが悪いです。

お値段が安いモデルにはclaude-3.0-haikuとかgpt-4o-miniとかgemini-1.5-flashとかがありますが、こいつらは割と能力が低いです。

そのため、トレードオフが存在していて、ずっと頭を悩ませ続けていたんですが、gemini-2.0-flashは、能力が高く、値段もすごく安いため、トレードオフしなくてもいいのでは？と希望を持てる状況になりました。

なお、gemini-2.0-flashと3.5-sonnetのどっちが賢いか？は特に検証していません。たぶん後者の方が間違いなく賢いとは思うけどAPIで使う範囲内であれば、sonnetじゃないと難しかったものも、リプレイスできるなという手応えです。

APIとしてではなく、対話AIアプリGeminiで2.0 Flashを使うなら、割とアホの子です。これはもうsonnetに完敗ですね。特にロングコンテキストになったときの指示追従能力の低さが論外です。

対話型AIで対話相手とするには不適切だけど、作業員として、延々と事務処理をさせるのに向いている、というのが筆者によるGemini 2.0 Flashへの評価です。つまり、「指示の与え方」がものすごく重要になります。

おそらく、コスパの良い小さいモデルなんだろうなーと思っています。むしろ逆に小さなモデルで、ここまで性能が良くてすごいです。

先日VertexAIでも使えるようになった

元々Google AI Studio（Google DeepMind）では gemini-2.0-flash-exp というモデルを無料で試すことができます。このたび Gemini on VertexAI（Google Cloud）でも gemini-2.0-flash-001 というモデルが正式に使えるようになりました。

2/11現時点では、まだ us-central1 でしか使えないようなので、東京リージョンである asia-northeast1 とかで使うにはもう少し時間がかかるかもしれません。

実際に使いこなしてみる

筆者は構造化出力を活用しています。先日のAI エージェント開発ハッカソン参加記事：Gemini 2.0 Flash で技術文書分析ツール「Tascario」を作ってみたで使っていたテクニックを紹介します。

Zennなどからとってきた技術記事や、適当な記事を手元に用意します。

食わせる文章はLLM系以外がおすすめです。 特にプロンプトが多めに含まれていると、指示追従能力に影響を与えることがあるため、評価にふさわしくないことがあるためです。

逆にいうと、一通り検証したあとは、そういった記事を食わせたときに指示が壊れないか？というテストもすべきですね。

プロンプト

用意するプロンプトは次の通りです。

  const prompt = `Contentは文章である。ただし不完全でノイズを含んでいることがある。これは日本人向けなので、必ず日本語で出力せよ。

1. Contentは必ずその文章の持つ主張があり、主張はこの文章全体で情報量が多いはずである。まずは主張を特定せよ
2. 主張を元にdescriptionを作成せよ
3. descriptionを元に、主張をすべて検証せよ
4. ノイズを取り除いた上で、残りの分析結果を日本語で出力せよ

<Content>
${content}
</Content>

※サイト全体で有効な情報、広告、広いアナウンスなどはノイズである可能性が高い。`;

重要なのはスキーマ定義です。

  const schema: ResponseSchema = {
    type: SchemaType.OBJECT,
    properties: {
      ...
    },
    required: [...]
  }

このような形式です。

中間分析

最終的に、gemini-2.0-flashの本当の力を発揮させるために、まず中間分析をします。

properties の中に以下のコードを記述します。

      claims: {
        type: SchemaType.ARRAY,
        description: "主張を出せる限り出し尽くす",
        items: {
          type: SchemaType.OBJECT,
          properties: {
            text: { type: SchemaType.STRING, description: "主張" },
            source: { type: SchemaType.STRING, description: "情報源" },
            evidence: { type: SchemaType.STRING, description: "根拠" },
            certainty: {
              type: SchemaType.STRING,
              description: "確からしさ",
            },
          },
          required: ["text", "evidence", "certainty"],
        },
      },
      description: {
        type: SchemaType.STRING,
        description: "文書の要約を1000文字程度で",
      },
      claimValidation: {
        type: SchemaType.ARRAY,
        description: "descriptionを元に、主張を検証しなおす",
        items: {
          type: SchemaType.OBJECT,
          properties: {
            claim: {
              type: SchemaType.STRING,
              description: "検証する対象の主張",
            },
            summaryDescription: {
              type: SchemaType.STRING,
              description: "descriptionとの関連性を説明せよ",
            },
            reader: {
              type: SchemaType.STRING,
              description: "それは読者が知りたいことか？",
            },
            isRelevant: {
              type: SchemaType.ARRAY,
              description: "この文章の主題にそっているか？",
              items: { type: SchemaType.BOOLEAN },
            },
          },
        },
      },
      noises: {
        type: SchemaType.ARRAY,
        description: "除去すべきノイズ",
        items: { type: SchemaType.STRING },
      },

プロンプト指示の1〜3をさせています。

まずは claims でこの文章における主張を出せる限り、出させます。そのための SchemaType.ARRAY であり、その主張に関して詳細に考えさせるための SchemaType.OBJECT です。どういう主張で、情報源はどこで、根拠は何か？確からしさを考えさせています。

この情報源や根拠は、モデルによってはハルシネーションを生じたり、指示の仕方に左右されがちですが、gemini-2.0-flashが賢いため、このシンプルすぎる指示でも十分に従ってくれます。もっと頭の良くないモデルだと、プロンプト全体に「ハルシネーションしないで」とか、指示に「Contentに含まれる文字列のみを使って」などが必要になることがあります。

またこれだけ賢いと、複数の情報源（URLと内容のセット）を与えると、どのURLの内容を参照した、みたいな情報も高い精度で取り出せるでしょう。

次にそれをもとに description を作らせます。

この二つを元に claimValidation で、それぞれの claims が、主題に沿っているか、読者の知りたい情報かをバリデーションします。

最後に noise として、文章に含まれている余分な要素を出力させます。

これで、中間分析ができます。

中間分析のメリットは gemini-2.0-flash の、ロングコンテキスト下での指示追従能力低下を抑制できることです。一般的にLLMはコンテキストの先頭と最後の影響を一番受けます。LLMの仕組みを思い出してほしいんですが、LLMは、トークナイズされた文章を受け取って、続きのトークンを1つ出力します。そのため、ある1トークンを出力するときの「最後」というのは、自分が出力した内容そのものの影響を受けます。

つまり対話型アプリで単純な指示を出しているとアホの子になってしまう現象を、中間分析させることで回避しています。中間分析を、CoT（Chain of Thought）の一種と考えることもできるでしょう。

同じようなことは、構造化出力じゃなくて、XML出力とかでも可能で、Clineのプロンプトや、ClaudeのArtifactsのシステムプロンプトではXMLタグを出力させて、思考をさせていますが、筆者的には構造化出力を活用するのが一番楽なので、半ば手癖で書いています。どれを使うと一番効果的かはモデルによっても違うかもしれません。

なお、ここではやっていませんが、指示や制約条件を復唱させると、指示に従ってくれない子を「ねじふせ」やすくなります。

最終成果物

最終成果物は次の通りです。

      keywords: {
        type: SchemaType.ARRAY,
        description: "文書の重要なキーワード",
        items: { type: SchemaType.STRING },
      },
      knowledge: {
        type: SchemaType.ARRAY,
        items: { type: SchemaType.STRING },
        description: "この文書から得られる知識・知見",
      },
      readingContexts: {
        type: SchemaType.ARRAY,
        items: { type: SchemaType.STRING },
        description: "この文章が役立つシチュエーション",
      },
      insights: {
        type: SchemaType.ARRAY,
        description: "この文章から得られる洞察",
        items: { type: SchemaType.STRING },
      },
      summary: {
        type: SchemaType.STRING,
        description: "忙しい人向けに3000文字程度で分かりやすく解説せよ",
      },
      title: {
        type: SchemaType.STRING,
        description: "文書のタイトル",
      },
    },

AI Agent Hackathonのために作ったTascarioでは、記事を与えると、その記事をいま読むべきか？をさくっと判断できる材料を出力したいので、「この文章から得られる知識・知見」「この文章が役立つシチュエーション」「この文章から得られる洞察」などが重要になってきます。それらを出力させています。

ここまでのプロンプトとスキーマ定義をVertexAIのgenerateContentに食わせます。

    const config = {
      projectId: "hoge",
      location: "us-central1",
      modelId: "gemini-2.0-fash-001"
    };

    const vertexAI = new VertexAI({
      project: config.projectId,
      location: config.location,
    });

    const model = vertexAI.preview.getGenerativeModel({
      model: config.modelId,
    });

    const result = await model.generateContent({
      contents: [{ role: "user", parts: [{ text: request.prompt }] }],
      generationConfig: {
        responseMimeType: "application/json",
        responseSchema,
      },
    });

    const response = result.response;
    const generatedText = response.candidates[0].content.parts[0].text;

このようなコードです。 generateContent を呼び出すときに generateConfig で responseMimeType: "application/json"が指定されていると、JSONが帰ってくる構造化出力が実現できます。

Google Cloudのサービスであり、その枠組みで認証をするために、サービスアカウントのJSONをローカルにダウンロードした状態で環境変数 GOOGLE_APPLICATION_CREDENTIALS で、そのJSONファイルへのパスを指定する必要があります。

結果

せっかくなので AI エージェント開発ハッカソン参加記事：Gemini 2.0 Flash で技術文書分析ツール「Tascario」を作ってみた の記事を食わせてみましょう。

サマリーはさほど違いはないんですが、大きく違うのは読むべき時（readingContext）や得られる知見（knowledge）です。

ちょうど、Tascarioでは「文章を読むべきかどうかを即座に判断する材料」が重要なのでこの二つがキーとなります。

中間分析なしの「読むべきとき」

大量の技術文書や情報を効率的に処理したいとき

AIとの会話内容を整理・分析したいとき

必要な情報を迅速に検索したいとき

思考を整理し、アイデアを発展させたいとき

個人の知識管理システムを構築したいとき

AI技術を活用した情報収集・分析ツールに興味があるとき

中間分析ありの「読むべきとき」

技術文書や記事を効率的に分析・検索したい時

大量のLLMの会話ログを整理・分析したい時

情報過多な状況で、必要な情報を見つけ出すのに苦労している時

AI技術を活用した情報収集・整理ツールに関心がある時

Gemini 2.0 FlashやVertex AIの活用事例を知りたい時

この通り、中間分析を入れるとより的確になります。「Gemini 2.0 FlashやVertex AIの活用事例を知りたい時」が入っているのは得点が高いですね。

中間分析なしの「読んで得られる知見」

Tascarioは、Gemini 2.0 Flashを活用した技術文書分析ツールである

Tascarioは、文書の取り込み、分析、検索、ノート作成機能を備えている

Tascarioは、GCP上で動作し、Cloud Run, Firestore, Vertex AIなどのサービスを利用している

Tascarioのアーキテクチャは、T3 Stackをベースとしており、フロントエンドとバックエンドはtRPCで通信している

Tascarioは、embedding技術を用いて文書の類似度を計算し、関連する情報を検索する

Tascarioは、ノート機能を通じて、思考の整理やAIによるサジェストを支援する

中間分析ありの「読んで得られる知見」

Gemini 2.0 Flashを用いた技術文書分析の手法

Embeddingによる文書のベクトル化と類似度計算

Google Cloud PlatformにおけるAIアプリケーションの構築

tRPC APIによるフロントエンドとバックエンドの連携

情報過多な状況における課題と、それを解決するためのAI技術の応用

この通り雲泥の差になっています。中間分析なしだとTascarioそのものの機能説明に終始していて、読者にとって読んで得られる知見ではありませんが、中間分析ありだと、ちゃんと読者にとって意味のある情報になっています。

また、ノイズが多い記事の分析をさせたとき、中間分析がないとノイズだらけの結果になります。

たとえば https://dic.pixiv.net/a/ジト目マチュ は、内容がかなりシンプルすぎて、他の情報が多い記事です。

これを分析させたとき、中間分析をしてノイズ除去をした方が的確になります。

中間分析ありの、「読んで得られる知見」と「読むべきとき」が次の通りです。ちゃんと、「ジト目マチュ」という極めてニッチな文章を正しく表現できていると思います。

{
  "knowledge": [
    "ジト目マチュは、『機動戦士Gundam GQuuuuuuX』の主人公アマテ・ユズリハの萌え要素である。",
    "ジト目マチュは、pixivでイラストや小説が投稿されている。",
    "ジト目マチュは、専用タグが作られるほど特徴的なキャラクターである。"
  ],
  "readingContexts": [
    "『機動戦士Gundam GQuuuuuuX』のファン",
    "ジト目属性のキャラクターに興味がある人",
    "pixivなどのイラスト投稿サイトで二次創作を楽しむ人",
    "アニメや漫画のキャラクターの属性について知りたい人",
    "特定のキャラクター属性に特化したタグの利用状況を知りたい人"
  ],
}

中間分析なしだと、次のように、大分ボヤけてしまいます。「ジト目マチュ」というコアテーマから外れたものになります。

{
  "knowledge": [
    "「ジト目」は、キャラクターの表情の一種であり、アニメやマンガなどの文脈で特定の感情や性格を表すために使用される。",
    "pixivは、イラストや小説などの創作物を共有・公開するためのオンラインプラットフォームである。",
    "機動戦士Gundam GQuuuuuuXは、ガンダムシリーズの一作品であり、特定のアニメシリーズまたは関連作品を指す。",
    "萌え要素とは、キャラクターの魅力的な特徴であり、ファンがそのキャラクターに愛着を持つ理由となる。",
    "タグは、オンラインコンテンツを分類し、検索を容易にするために使用されるキーワードまたはフレーズである。"
  ],
  "readingContexts": [
    "アニメやマンガのキャラクターデザインに関心がある人",
    "pixivなどのオンラインアートコミュニティの動向を知りたい人",
    "特定のキャラクターのファンアートや二次創作を楽しみたい人",
    "ガンダムシリーズの新しい作品やキャラクターについて情報を探している人",
    "「ジト目」のような特定の視覚的特徴が、キャラクターの魅力にどのように貢献するかを理解したい人"
  ],
}

ノイズの方が多い記事から、正しく「ジト目マチュ」を取り出させているすごさがわかるでしょうか？最初から人間が「ジト目マチュ」を出してくれといえば従ってくれますが、それでは汎用的なプロンプトではありません。かといって、「タイトル」から取り出してくれ」という絞った指示だとそこに的確に書かれていないものは取り出せないでしょう。

中間分析があることによって、LLMの持つ柔軟な知性が的確に発揮されたことがわかるでしょう。

出力形式

ちゃんとJSONで出力が保証されているようでその点は楽でした。実験途中では古いモデルだったり、オプション指定ミスとかで、JSON以外の文字列が混じって苦労しました。なお、その名残がHackathonのコードである https://github.com/erukiti/tascario-public に残っていますが、そのコードは不要だったはずです。

発展

本当はTypeScriptとの相性の良いzodスキーマを使いたかったです。zodスキーマをJsonSchemaに変換するライブラリはあるので、食わせてみたんですが、JsonSchmeaそのものではないようでダメでした。

まとめ

gemini-2.0-flash はAPIで使うこと前提で十分な性能を発揮し、値段が安いため、コスパのよい優秀なモデル

ただし、プロンプトの与え方次第で性能が大きく変わる

中間分析をさせることで、最終成果物の精度が大きく変わる（構造化出力を駆使したCoT）

構造化出力は指示がしやすく、かつ情報が取り出しやすい

erukiti

株式会社AlgomaticのAIエンジニャー

バッジを贈って著者を応援しよう

バッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。

Discussion

erukiti

株式会社AlgomaticのAIエンジニャー

バッジを贈るとは

目次

この記事で得られる知見

gemini-2.0-flashの衝撃

先日VertexAIでも使えるようになった

実際に使いこなしてみる

プロンプト

中間分析

最終成果物

結果

出力形式

発展

まとめ

エンジニアのための 情報共有コミュニティ