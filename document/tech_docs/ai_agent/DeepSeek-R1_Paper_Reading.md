 # DeepSeek-R1の論文読んだ？【勉強になるよ】

```markdown
目次

1. はじめに
2. 本論文の興味深いところ
3. 本論文の流れ
    1. 前提のモデル構造
    2. DeepSeek-R1-Zeroを強化学習を用いて作る
    3. DeepSeek-R1をSFT+強化学習を用いて作る
    4. 小型モデルへの蒸留を試す
4. DeepSeek-R1-Zeroを作るまで
    1. 元々のアーキテクチャ
    2. すごいぞ！強化学習
    3. そもそもLLMで強化学習とは？
    4. ZeroのSystemプロンプト
    5. Zeroの「Aha Moment」
    6. DeepSeek-R1-Zeroのまとめ
5. DeepSeek-R1への道
    1. DeepSeek-R1-Zeroの課題
    2. 課題対処への道
    3. DeepSeek-R1のまとめ
6. 結果
7. 蒸留
    1. 蒸留 vs 強化学習
8. 今後の展望
9. まとめ
10. 参考文献
```
2025/01/27に公開

# はじめに

タイトル通りDeepSeek-R1の論文を読んだので、少しでもわかりやすく説明できればと思います。
まだ論文を読んでいない方は、ぜひ原文も読んでみてください！

難しい数式や概念がほとんど使われていないため、論文の中では全然読みやすい方だと思います。読み物として読めるレベルです。
[https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek%5FR1.pdf](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek%25FR1.pdf)

とはいえ、本記事では細かい部分など間違っている部分もあるかもしれないので、ぜひ誤りがあったら、こそっと教えてください！

!

実は、本論文を解説する上で、著者らが公開しているもう一つの論文が大きく関連しています。
そちらの論文の内容も若干解説を入れております。

「DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models」

本記事でも少し触れますが、上記の論文も非常に勉強になるため、ぜひ読んでみてください。

!

本記事は、DeepSeek-R1の論文とDeepSeekMathの論文を読んだ私の理解をもとに記載しています。

本論文で使われている技術に関しては、ある程度の知識を持っているので、大きくは外していないとは思いますが、私の主観も入っている部分もありますので、ご了承ください。

また、DeepSeek-R1の論文が公開される前に、小型モデルに対して同様の実験（強化学習）をしていたグループがあるようです。
そちらのレポートは下記になります。
[https://hkust-nlp.notion.site/simplerl-reason](https://hkust-nlp.notion.site/simplerl-reason)

意図せず、DeepSeek-R1-Zeroの再現実験のようなレポートになっていますが、レポートの著者はDeepSeek-R1論文の公開前から実験していると主張しています。
こちらも非常に興味深かったため紹介です。

# 本論文の興味深いところ

本論文は、大きく分けて3つの構成でできています

* **強化学習による思考能力の強化**
    * LLM（DeepSeek-V3-Base）に対して「強化学習のみ」を適用させたところ、強力な思考能力を獲得した「DeepSeek-R1-Zero」が作成されました。
    * 本論文のメインストリームであり、非常に興味深い示唆が得られる部分です。
* **SFTによるさらなる強化**
    * DeepSeek-V3-Baseに対して、コールドスタート問題を解消するために、CoT（Chain of Thought）データセットによるSFT（教師付きファインチューニング）を実施
    * その後、強化学習を適用させることで、o1レベルの性能を獲得した「DeepSeek-R1」が作成されました。
    * 非常に強力なLLMを獲得するまでの反復学習などの試行錯誤などを学ぶことができます。
* **蒸留による小型モデルの性能強化**
    * 上記で作成されたDeepSeek-R1により、他社の小型オープンソースLLMを蒸留しました。
    * その結果、元の性能を大幅に上回る、非常に高い性能の小型LLMが得られました。

その中でも私が特に面白いなと思った要素は下記の要素になります

* **強化学習手法「GRPO」すごい！**
* **ルールベース報酬で思考能力が向上するのがすごい！**
* **aha moment（ハッとする瞬間）面白い！**
* **蒸留vs強化学習の比較実験から重要な示唆が得られた！**

# 本論文の流れ

### 前提のモデル構造

そもそも、大前提として、この論文は「DeepSeek-V3-Base」という強力なLLMの性能を改善する論文です。
これ単体でも、GPT-4oレベルの能力を持っているオープンソースLLMになります。

本論文のモデルアーキテクチャは「DeepSeek-V3-Base」と完全に同じものを利用します。
（つまりモデルアーキテクチャをかえずに、学習方法を工夫してLLMの性能を上げる挑戦になります）

### DeepSeek-R1-Zeroを強化学習を用いて作る

その上で、まずは、「DeepSeek-V3-Base」に対して、大規模な強化学習のみを適応させることで、強化学習の可能性について探究する話が始まります。
結果として、「DeepSeek-R1-Zero」ができます。
「DeepSeek-R1-Zero」は、SFT（教師付きファインチューニング）をしなくても、強化学習のみで圧倒的な思考能力を取得できることを示す興味深いモデルです。

### DeepSeek-R1をSFT+強化学習を用いて作る

その後、あまりにも「DeepSeek-R1-Zero」の性能が良かったことから、さらなる探究が進められました。

この時点で、「DeepSeek-R1-Zero」には下記の課題がありました。

* 学習初期段階にて、強化学習の学習が不安定であり、収束に時間がかかる（コールドスタート問題）
* 思考能力は高いが、言語が混在（日本語で話していたら急に中国語になるなど）し、人間が読みにくい
    * 結果、高度な思考が必要なタスクには強いが、一般的タスク（ロールプレイやライティングなど）には使いにくいという評価

そこで、「DeepSeek-V3-Base」に対して、少量のCoT（Chain of Thought）データセットを用いて、SFT（教師付き微調整）を行います。このデータは、著者らが収集・構築したものになります。
その後、SFT後のモデルに対して、大規模強化学習を行います。
このとき、「DeepSeek-R1-Zero」を作成した際の報酬に加え、「言語一貫性報酬」を導入して、上述した言語の不一致の問題を緩和させます。

その後、得られたモデル（チェックポイント）を利用して、さらなるSFT（教師付き微調整）学習データを収集し、追加SFTを行います。
さらに、その後、改めて２段階目の大規模強化学習を実施することで、最終的に「DeepSeek-R1」が得られます。

### 小型モデルへの蒸留を試す

最後に、小型モデルに対する「DeepSeek-R1」による蒸留が試されています。
その中で、小型モデルに対して、「DeepSeek-R1」による蒸留を実施するのと、「DeepSeek-R1-Zero」と同様の強化学習手法で思考能力を強化するのと、どちらが性能向上に寄与するのかを調査してくれています。

これが非常に興味深いです。

# DeepSeek-R1-Zeroを作るまで

では、ここから論文の詳細な解説に入っていきます。

まずは、「DeepSeek-R1」の前段階である「DeepSeek-R1-Zero」を作るまでの手法を解説します。

**前段階と侮るなかれ、「DeepSeek-R1-Zero」を作るまでの手法と示唆にこそ、本論文の貴重な示唆が含まれています。**

## 元々のアーキテクチャ

今回作る「DeepSeek-R1-Zero」や「DeepSeek-R1」は、新しいモデル構造を採用して、性能を向上させたわけではありません。

元々のアーキテクチャは「DeepSeek-V3-Base」というアーキテクチャになります。
「DeepSeek-V3-Base」は、681Bのモデルパラメータを持つ大規模言語モデルで、性能はGPT-4oに匹敵する非常に高性能モデルありながら、モデル重みが完全に公開されているオープンソースモデルです。

「DeepSeek-V3-Base」に対して、Instructionチューニングを施したchatモデルに関しては、APIでも公開されています。
非常に安価に使えるにも関わらず、GPT-4oに匹敵する性能を誇ることから、人気のモデルになっております。

!

安価なモデルではありますが、APIの提供元は中国の企業ですので、データの取り扱いなどは十分注意してください。
基本的には、API利用の際には、収集されて困るような情報は入力しないようにするのが無難です。
（オープンソースモデルをローカルで動かす場合は、おそらく心配ありません）

本論文のすごいところは、モデル構造を変えずに、「DeepSeek-V3-Base」に対する学習方法を工夫しただけで、OpenAI o1に匹敵する性能のLLMを開発していることです。
具体的には、大規模強化学習をLLMに対して適用することで、「DeepSeek-R1-Zero」を開発します。

## すごいぞ！強化学習

### 強化学習って何？

そもそも「強化学習って何？」という方もいらっしゃると思うので、簡単に説明します。
（以前、スイカゲームを強化学習でプレイしたこともあるので、いずれ詳しい解説記事を作る予定ですが、ここではざっくり説明します。）

強化学習とは、一言で言えば「一連の行動の流れ（シークエンス）によって得られる、累積報酬の期待値を最大化する」ことを目指す学習手法です。
ある行動をとった結果がすぐにはわからず、時間が経ってから影響が現れるような問題に適しています。
専門的に言うと、「マルコフ決定過程（MDP）」が成立する問題で活用されます。

### テトリスで考える

例えば、「テトリス」を考えてみましょう。

テトリスは、上から「ミノ」と呼ばれる、さまざまな形のブロックを落として、横一列を綺麗に埋めてブロックを消去するゲームです。

横一列がすべて揃うと、その列が消えて、「ポイント」を獲得できます。
逆に、列を消せずに、ブロックが一定の高さまで積み上がると「ゲームオーバ」になります。

テトリスを強化学習で解こうとすると、「どうすれば最も多くのポイントを得られるか」をAIに学習させることが重要になります。
単純に列を揃えて消すだけではなく、「次の手」や「その次の手」を見据えて、より良い結果につながるようにミノを配置することも必要です。
しかし、通常の学習方法(よくある入出力を近似させる学習方法)では、目の前の結果だけに注目してしまい、将来的な高得点につながる手を学習させることは難しいです。

**そこで役立つのが「強化学習」です。**

強化学習では、ゲームの最初から最後までの流れを通して、「どう行動すれば最終的に得られる得点を最大化できるか」を学習できます。
このように、途中の行動の良し悪しがすぐにはわからなくても、最終的なゴール（得点の最大化）が設定できる問題に対して、強化学習は非常に効果的です。

補足

（細かい話なので、興味ない方はスキップで大丈夫です）

テトリスは、強化学習で効率的に学習できます。
これは、テトリスが「マルコフ決定過程」が成立する問題設定だからです。


マルコフ決定過程が成立するために必要な条件は下記です。


1. 状態にマルコフ性が仮定できる
マルコフ性とは、次の状態が現在の状態にのみ依存し、過去の状態には依存しない性質です。
テトリスでは、過去から現在までに、どのようにゲームが進んでいるかに関係なく、盤面（積み上がったブロックの形と次に降ってくるミノの種類）が同じであれば、最善手や最悪手は同じになります。
このような性質を「マルコフ性」と言います。


2. 報酬、行動が定義できる

テトリスでは、AIに「その手が良い手かどうか」を教えるための「報酬」と、「AIが取れる手」である「行動」を定義できます。

「行動」は、ゲームパッドの入力と同様に、回転や移動を考えて、各フレームごとに、ボタン入力をするかしないかで定義すれば良さそうです。
「報酬」は、列を消去した時に発生するポイントで定義できれば良さそうです。

そうすると、あるtステップ目における、以降の累積報酬G\_tは以下のようになります。
...(about 538 lines omitted)...
結果として、問題の正当に近づくようなtokenを出力する能力、すなわち問題をステップバイステップで解く能力が磨かれることになります。

このように、数学という問題の答えを合わせるように報酬設計をすることで、勝手に思考能力が強化される、というのは非常に面白いなと思います。

#### （補足）データセットの作り方（の考察）

本論文での、強化学習に利用するデータセットは、おそらく上述した論文の前半に記載されている数学データセットです。

!

強化学習に利用したデータセットに関しては、本論文で言及されていなかったと思うので憶測です。間違っていたら教えてください。

本論文の主要部分ではないですが、興味深いため紹介します。


引用：DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

**1\. Train a FastText Model**
まず初めに、「OpenWebMath」という高品質の数学のウェブテキストのデータセットを初期のシードコーパスとして利用します。
このコーパスから50万のデータを正例、 Common Crawlデータセット（大規模なWebページコーパス）から取得した50万のデータを負例として、fastTextモデルを学習します。
こうすることにより、fastTextモデルを利用して、Webページから、それが数学コンテンツか、非数学コンテンツかを分類することができます。

**2\. Recall Math-Related Webpages From Common Crawl**

学習した、fastTextモデルを利用して、Common Crawlデータセットから数学のWebページを取得します。
このとき、低品質な数学コンテンツを除外するために、 fastText モデルによって予測されたスコアをもとにランク付けした、上位のスコアのページのみを取得します。

**3\. Discover Math-Related Domains**

この方法で取得されたWebページは、数学の問題ではありますが、偏りがあります。
なぜなら、初期の学習に利用された「OpenWebMath」データセットには、少数のデータしかなく、さらに上位のスコアのWebコーパス（類似度の高いWebコーパス）しか収集していないため、「OpenWebMath」データセットと似たような数学Webページしか、収集できていないからです。

そこで、上記の方法で収集したWebページの親ドメインに着目します。
そして、親ドメイン内のWebページのうち、10%以上のWebページが、今回の方法で収集されている場合、親ドメイン自体が数学に関連するドメインであると、考えることができます。

!

例えば、「mathhoge.com」の配下のページの10%以上がfastText モデルによって収集されていた場合は、「mathhoge.com」自体を数学コンテンツであると分類します。

あとは、この親ドメイン内の、まだ収集されていないWebページから、高品質な数学Webページを収集することで、幅広い数学のコンテンツを収集できそうです。

**4\. Annotate Math-Related URL Path From Labelers**

分類されたドメイン内の数学コンテンツに対して、「手動」でURLに注釈をつけ、注釈をつけたもののうち、またシードコーパスに収集されていないWebページを、シードコーパスに追加します。

この方法を繰り返すことで、高品質かつ多様な数学データセットを収集することができます。
著者らはこのデータ収集を4回繰り返したのちに、3,550万の数学Webページ、合計1200億トークンのコーパスを完成させました。

4回の繰り返しの段階で、データのうち98%が3回目までの繰り返しですでに収集されていたことがわかったため、4回で繰り返し作業を終了したとのことです。

### （補足）DPO（Direct Preference Optimization）との違い

DPO（Direct Preference Optimization）という手法があります。
こちらは、強化学習を利用して、LLMに好ましい回答を学習させるRLHF（Reinforcement learning from human feedback）と同様の効果を、強化学習を使わずに学習させることができる手法です。

強化学習をしないため、報酬モデルも状態価値モデルも必要なく、単純に「プロンプトx」と「好ましい回答y^+」「好ましくない回答y^-」の3つが組となっているデータセットDを用意すれば、あとは\\pi\_\\phi(y^+|x)、\\pi\_\\phi(y^-|x)、\\pi\_{ref}(y^+|x)、\\pi\_{ref}(y^+|x)を計算して、損失関数に落とし込むことで、学習が可能になります。

このように、「報酬モデルも状態価値モデルも必要ない」の部分が、本論文で提案している「GRPO（Group Relative Policy Optimization）+ルールベース報酬」の手法の利点と近いです。

ただし、DPOでは、上述した通り、「プロンプトx」と「好ましい回答y^+」「好ましくない回答y^-」の3つが組となっているデータセットが必要です。
逆に言えば、このデータセットの範囲内でしか学習ができません。

一方で、著者らが提案する「GRPO（Group Relative Policy Optimization）+ルールベース報酬」の手法では、数学などの答えが一意に決まり、ルールベースで正解不正解を判断できるデータセット（つまり、数学データセット）であれば、「プロンプトx」と「回答y」の組だけで、学習が可能です。

**したがって、DPOよりもはるかに多くのデータを用いて学習ができます。**
これが、今回の提案手法の利点になりそうです。

### 著者らの執念

著者らは、LLMを強化学習に適用する上で、計算量的に障害になる報酬モデルと状態価値モデルを利用しない斬新な手法を提案しました。
これにより、圧倒的な計算効率にて、強化学習を適用させました。

また、似た効果を持つ手法としてDPOという手法があります。
ただし、DPOでは、利用できるデータセットが制限されるため、大規模な学習は困難です。

しかしながら著者らの手法は、ただ数学のように一定の推論の結果、答えが一意に定まるような問題を解かせ続ければ、ルールベース報酬による強化学習で、思考機能を強化させることができてしまっています。

**絶対に、学習データの量を減らさずに、計算量を徹底的に削減してやろうという著者らの執念を感じるとともに、こういう工夫に積み重ねが、高性能と圧倒的に格安なAPI料金の両立を達成しているのだろうなと感じました。**

## ZeroのSystemプロンプト

続いては、少し話が変わり、「DeepSeek-R1-Zero」に与えているsystemプロンプトについて紹介します。
このように、systemプロンプトが公開されているというのは、非常に勉強になりますね。とても嬉しいです

systemプロンプトは下記です。

A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: prompt. Assistant:

訳）
ユーザーとアシスタントの会話。ユーザーが質問し、アシスタントがそれを解決する。アシスタントは、まず頭の中で推論プロセスを考え、それからユーザに答えを提供します。推論過程と答えはそれぞれ<think> </think>タグと<answer> </answer>タグで囲まれます。つまり、<think> 推論過程はここ</think> <answer>答えはここ</answer>です。ユーザー：プロンプト アシスタント：

上記部分の「prompt」の部分は、ユーザが入力したプロンプトの内容に書き換えられます。
このように、systemプロンプトにおいて<think> </think>タグを利用して、その中で思考をさせる形になっています。

## Zeroの「Aha Moment」

「Aha Moment」とは、辞書で調べると「〔問題の答えが突然ひらめいたときなどの〕なるほど！と思う瞬間」とのことです。

実は、DeepSeek-R1-Zeroの出力において、それが発生したそうです。
数学の問題を解く途中で、下記のような出力が見られました。

Wait, wait. Wait. That's an aha moment I can flag here.
Let's reevaluate this step-by-step to identify if the correct sum can be · · ·
We started with the equation:

訳）
待て、待て。待ってくれ。これはハッとさせられる瞬間だ。
正しい和が - - - - になるかどうかを確認するために、ステップ・バイ・ステップで再評価してみよう。

これは恐ろしいことです。
モデルは途中まで推論を実施しながら、途中で自身の出力結果を見直し、さらにもう一度ステップバイステップで検討をするという、高度な問題解決能力を獲得しています。
（自分の考えを見直して、さらに良い戦略を発見する。これができる人間がどれくらいいるのだろうか）

何度も記載していますが、DeepSeek-R1-Zeroを学習する際に、問題の解き方を明示的にモデルに教えることや、思考部分を強化させるような報酬は与えていません。
ただ、数学の問題に正解することで得られる報酬を与えるだけで、モデルが自律的に高度な問題解決戦略を獲得しています。
（もちろん、推論の途中で、検算を行うなどのデータセットで学習もさせていません。もしかしたら、DeepSeek-V3-Baseを学習させるときのSFTデータセットには含まれているかもですが。）

これこそが、本論文で何度も述べている、「強化学習の可能性」です。
強化学習は（そのほかの学習手法とは異なり）（人間にとって）予想外で、かつ洗練された結果を導き、人工知能が新たなレベルの知性を手にいれるための核となる技術になりそうです。

## DeepSeek-R1-Zeroのまとめ

DeepSeek-R1-ZeroはDeepSeek-V3-Baseに対して、「GRPO+ルールベース報酬」の強化学習「のみ」を適用し、圧倒的な思考能力を獲得したモデルになります。

DeepSeek-R1-Zeroを学習する際に、問題の解き方を明示的にモデルに教えることや、思考部分を強化させるような報酬は与えていません。
ただ、数学の問題に正解することで得られる報酬を与えるだけで、モデルが自律的に高度な問題解決戦略を獲得するという、とても興味深く、恐ろしい結果が示されています。

# DeepSeek-R1への道

さて、ここまで、「DeepSeek-R1-Zero」について、たくさん語ってきましたが、とても魅力的でしたよね？
あくまで、「DeepSeek-R1-Zero」は「DeepSeek-V3-Base」に対して、強化学習のみを適用させて、強化学習の可能性を探ったモデルでした。

その結果、こんなにも魅力的なモデル（DeepSeek-R1-Zero）ができてしまったら、、、
**SFTも入れて完全体のモデルを作りたくなりますよね？？？**

というモチベーションで「DeepSeek-R1」は作られています。

もし、DeepSeek-R1-Zeroの性能がそんなに高くなかったら、今騒がれているDeepSeek-R1は作ろうという発想にすらならなかったと思います。

## DeepSeek-R1-Zeroの課題

さて、非常に高性能で、示唆に富んだDeepSeek-R1-Zeroでしたが、課題はいくつかあります。
それは下記です。

* 学習初期は強化学習の学習が不安定で収束に時間がかかる（コールドスタート問題）
* 思考能力は高いが、言語が混在（日本語で話していたら急に中国語になるなど）し、人間が理解しにくい
    * 結果、高度な思考タスクには強いが、（ロールプレイなどの）一般的なタスクには使えないという評価

そこで、上記の課題を解決する旅に出ます。

## 課題対処への道

DeepSeek-R1-Zeroでは、ベースモデルであるDeepSeek-V3-Baseに対して、そのまま上述した強化学習を用いて学習をしていました。

しかし、もともとDeepSeek-V3-Baseは長考モデルとして学習されたモデルではないため、いくらsystemプロンプトで長考をさせる制限（`<think>`タグ）を入れたとしても、強化学習の初期フェーズは不安定な学習になってしまいます。
（例えば、「そもそも適切な形式で思考をしない」、「思考形式は合っているが、意味のある思考をしていない」、その結果として「数学の問題になかなか正答できないまま、報酬が得られず、学習が不安定になる」が考えられます）

そこで、学習初期のコールドスタート問題を防ぐために、DeepSeek-V3-Baseに対して、SFT（Supervised Fine-Tuning）を実施します。
そのために著者らは、少量のCoT（Chain-of-Thought）データセットを構築しています。

### CoTデータセットの構築+SFT

データセットの構築方法は下記が一例として提示されております。

* いくつかの長いCoT解答を提示し、モデルに長いCoT形式の回答を生成させてデータセットに追加する
* モデルに対して、自己反省や検証を行いながら、詳細な回答生成を促すようなプロンプトを送信し、その出力をデータセットに追加する
* DeepSeek-R1-Zeroの出力を、読みやすい形式に整理してからデータセットに追加する
* モデルの出力結果を人間が精査して、より適切なCoTデータの形に整理してから、データセットに追加する

上記の方法で、数千件のCoTデータを構築し、そのデータを用いて、DeepSeek-V3-BaseをSFTしています。

### CoTデータセットを用いることの利点

著者らは、下記の要素において、CoTデータセットを用いることに利点があると考えています。

**読みやすさ**
DeepSeek-R1-Zeroの一番の課題は、その出力結果が人間が理解するのが難しいことです。
回答には、複数の言語が混在していたり、マークダウン形式などの読みやすい形式が適切に使われていないことで、重要な情報が何かわかりにくい状態でした。

そこで、今回のデータセットを作成する際には、各回答の末尾に「要約文」を入れるようにすることで、人間が読みやすい形に出力させるようにしています。
具体的には下記のような出力形式で、全てのCoTデータセットは作成されています。 

`<reasoning_process>`の部分に、モデルの思考に該当する部分が、`<summary>`の部分に最終的な結論の要約が記載されます。

これにより、「DeepSeek-R1-Zero」にあった、重要な情報が何かわかりにくいという人間が理解するのが難しい問題を緩和します。
（言語の混在に関しての対処は後述します）

**ポテンシャル**
今回、強化学習のみを実施するのではなく、人間の事前知識に基づいて作成したCoTデータセットでSFTを実施しています。
また、SFTと強化学習を複数回反復して、繰り返し学習をしています。
その結果、DeepSeek-R1はDeepSeek-R1-Zeroよりも良い結果を得ることができました。

著者らは、このような反復的な学習こそが、長考モデル（reasoning models）を学習する上で良い方法であると主張しています。

### 推論思考の強化学習

CoTデータセットによりSFTしたDeepSeek-V3-Baseに対して、上述した（DeepSeek-R1-Zeroを学習した）強化学習手法を適用します。
これにより、モデルの思考能力が大幅に強化されます。

一方で、「DeepSeek-R1-Zero」には、言語不一致の問題がありました。
そこで、DeepSeek-R1-Zeroのルールベース報酬に、「言語一貫性報酬」を追加で導入しています。

この報酬導入で、実は、モデルの思考能力が若干落ちてしまうことが実験的にわかっています。
しかしながら、この報酬があることで、ようやく人間が理解しやすい文章を生成してくれるようになります。

### さらなる追加のSFT

上記の強化学習が収束したら、さらにデータを追加してSFTをやり直します。
ここでは、初期段階の、CoTに重きを置いた、コールドスタートを解消するための学習データだけにとどまらず、ライティング、ロールプレイといった汎用タスクの性能を向上させるため、他ドメインからのデータも合わせて収集します。

#### Reasoning Data

まずは、前段階と同様に思考能力を向上させるために、CoT学習データ（Reasoning Data）を取得します。

取得方法は以下です。

* 強化学習済みモデルの出力から、適切なものを収集して学習データに追加
    * 前段の強化学習では、ルールベースの報酬モデルを利用していたので、それが使える問題（数学など）のみを学習データとしていたが、次の強化学習では報酬モデルを作成するため、もっと広い範囲でCoTができるデータも学習データに追加
    * モデルの出力はまだ読みにくいところがあるため、さまざまフィルタリング

この結果、合計で600kのReasoning Data（学習データ）を収集しています。

#### Non-Reasoning Data

ライティングやQ＆A、翻訳などのNon-Reasoning Dataについては、DeepSeek-V3のSFTデータセットの一部を再利用して、学習データを収集しています。
上記のReasoning Data以外のタスク（Non-Reasoning Data）に関しては、DeepSeek-V3によって簡易的なCoTを実施して、回答を生成し、学習データとします。
ただし、本当に簡単なタスク（あいさつとか）に関しては、CoTを学習データに追加しません。

こうすることで、合計200kのトレーニングサンプルを最終的に収集したそうです。

上記の合計800kサンプルの学習データを利用して、DeepSeek-V3-Base（初回強化学習済み）を2epoch分SFTを実施しています。

### 最後の強化学習

最後に、モデルの思考能力をさらに磨くと同時に、モデルの無害性などを向上させることを目的に、強化学習を実施します。

思考タスクの強化学習に関しては、これまで説明したのと同様に、数学、コード、論理的推論の領域でルールベース報酬を利用して強化学習をします。
一方で、思考タスク以外の一般的なデータに対しては、ルールベースで回答の良し悪しを判断することはできないため、報酬モデルを別途DeepSeek-V3パイプラインをベースに作成しています。

## DeepSeek-R1のまとめ

以上より、DeepSeek-R1では、DeepSeek-V3-Baseに対して、SFTと強化学習を2回実施することにより構築されています。

SFTにより、強化学習のコールドスタート問題の解消と、モデル出力のユーザフレンドリー性の向上、一般的なタスクに対する性能向上に貢献しています。

# 結果

結果に関しては、他の方がさまざま検証していたり、話題になっているので詳しくは書きません。
ただ、DeepSeek-R1は、現在のオープンソースモデルの中では、最も高い性能と言ってよく、また、Open AIのo1モデルにも匹敵する性能だと言われています。

# 蒸留

最後に、蒸留モデルについての解説します。（論文でも言及されています）
DeepSeek-R1は671Bものモデルパラメータを持つため、一般的な家庭用PCで動作させるのは難しいです。

そこで、より小さなモデルパラメータのモデルに、DeepSeek-R1のような思考能力を与えるために、蒸留を実施しています。
（公式が、個人の利用者のことも考えて、家庭用PCで動作する思考モデルを開発してくれるのは非常にありがたいですね）

具体的には、Qwen2.5-Math-1.5B、Qwen2.5-Math-7B、Qwen2.5-14B、Qwen2.5-32B、Llama-3.1-8B、Llama-3.3-70B-Instructに対して、DeepSeek-R1から取得した800kのトレーニングサンプルを利用して、直接FineTuningする形で、蒸留を実施しています。

蒸留モデルに対しては、SFTのみを実施し、後段の強化学習は実施しておらず、蒸留モデルに対する追加の強化学習の探究は、著者らが実施する予定はないそうです。
（著者らは、強化学習を追加で実施することにより性能が上がると思っているが、その探究は、別のコミュニティに任せると記述しています）

## 蒸留 vs 強化学習

蒸留後の強化学習に関しては、著者らは実験していない、と記述していました。
では、小型モデルに対して、蒸留を実施する前に、強化学習を実施したらどうなるのでしょうか？
DeepSeek-V3-Baseのように、大幅に思考能力が強化されて、性能向上が期待できるのでしょうか？

それに関しては、著者らが実験してくれています。

ここでは、「Qwen-32B-Base」モデルを利用して、10kステップ以上の大規模強化学習を実施し、「DeepSeek-R1-Zero-Qwen-32B」を構築しました。
一方で、「Qwen-32B-Base」モデルに対して、蒸留を実施したモデルを、「DeepSeek-R1Distill-Qwen-32B」モデルとして、この二つのモデルを比較します。

その結果を比較すると、小型モデルに対する強化学習を適用した「DeepSeek-R1-Zero-Qwen-32B」よりも、小型モデルに蒸留を適用した「DeepSeek-R1Distill-Qwen-32B」モデルの方がはるかに高い性能でした。

**以下は私の主観ですが、**
以上の結果から、
**強化学習による思考能力の向上は、安易に小型モデルに試していい戦略でない**
ということが示唆されていると思います。

強化学習による思考能力の大幅な強化は、おそらくですが「DeepSeek-V3」という元々大規模かつ性能の高いモデルであったからこそ、達成されたのだと思います。

DeepSeek-R1-Zeroの課題でも説明した通り、今回のルールベース報酬では、正しい数学の解答を生成できないと永遠に報酬が獲得できません。
小型モデルでは、このコールドスタートの問題が、より致命的なレベルで発生するのだと思います。

したがって、思考モデルを作りたいからといって、小型モデルに対して、安易にルールベース強化学習を試すのはおすすめしません。
しかしながら、コールドスタート問題を解決できる戦略があるのであれば、有効な手段になると思われます。
（例えば、DeepSeek-R1で蒸留済みの小型モデルを利用する、最初から数学に強い小型モデルで強化学習を試すなどです。重要なのは強化学習のコールドスタートを解消できるかどうかだと思われます。）

（追記）
また、同様の実験を小型モデルに対して試したグループもいるようです。
そちらの実験レポートが下記になります。
[https://hkust-nlp.notion.site/simplerl-reason](https://hkust-nlp.notion.site/simplerl-reason)
（DeepSeek-R1-Zeroと同様に、「Aha Moment」が見られることもわかっており、非常に興味深いです）

ただ、こちらのレポートにおいても、ベースモデルとしては数学に特化したモデルを利用しています。
このことからも、ある程度の数学能力は必要なのだなと思います。

# 今後の展望

DeepSeek-R1は、さまざまなタスクにおいて、OpenAI o1に匹敵する性能を達成した、驚異的なオープンソースモデルですが、苦手としている部分もあります。
それが今後の展望に記載されています。
（今後の展望を見れば、DeepSeek-R1が苦手な部分を知ることができます）

それは以下の項目です。

* **一般的な能力**
    * 関数呼び出しやマルチターン対話、複雑なロールプレイング。、json出力において、DeepSeek-V3に及ばない
* **言語の混在**
    * 現在、DeepSeek-R1は英語と中国語に最適化されているため、そのほかの言語を利用した場合、言語の混在が発生する可能性がある
* **プロンプトエンジニアリング**
    * DeepSeek-R1はプロンプトに非常に敏感で、特にFewーshotプロンプトを利用すると、パフォーマンスが低下する。
    * 基本的には、問題を直接シンプルに記述し、例を提示しない（Zero-shotプロンプト）を利用することを推奨している
* **ソフトウェアエンジニアリングタスク**
    * コード生成やコード保管などのソフトウェアエンジニアリングタスクは、評価に時間がかかるため、強化学習の効率が悪くなることから、~~現時点では強化学習において、このようなタスクは学習していない。~~ 大々的には学習しておらず、一部のテスト可能な範囲でしか学習できていない。
    * 結果として、このタスクの性能は、DeepSeek-V3からほとんど改善していない。

!

（追記）
masaishiさんのコメントに基づき修正させていただきました。
私の読み間違いで誤解を与えてしまい申し訳ございません。

> 2.2.2\. Reward Modeling
> ・・・
> ・Accuracy rewards:・・・Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.

と記載のあるように、全くコード生成などを学習していないわけではなく、一部の、簡単にテストができる問題のみ学習しているようです。
ただ、そういう簡易的なタスクだけしか学習できていないため、性能向上は小さめということだと思われます。

上記の内容が、現時点での「DeepSeek-R1」の課題です。
ですが、今後のアップデートで改善していくとも記載されているため、現時点で不満があったとしても、上記に記載されている項目であれば、今後改善する可能性が高いです。

# まとめ

ここまで読んでくださった方、本当にお疲れ様でした。
（これでも説明はだいぶ短くした方なんです・・・本当は、もっと強化学習の部分とかを数式をもとに説明したかったです）

DeepSeek-R1の論文いかがでしたか？
私にとっては、非常に勉強になるような示唆や知見に溢れており、こんな論文を公開してくれて、かつオープンソースのモデルとしても公開してくれる「DeepSeek-AI」には感謝しかありません。

本記事が、皆様の理解の助けになり、一人でも多くの方が原論文に興味を持って、読んでくれましたら幸いです。
ここまで読んでくださってありがとうございました！

# 参考文献

本論文は、論文の中では読みやすい方だとは思いますが、強化学習に関しては、一定程度の基礎知識は要求されます。
そこで、本論文を読む上で必要な知識を収集できるおすすめの書籍に関して記載します。
（少なくとも私はこの書籍で学習しました）

（書籍のリンクはamazonアフィリエイトリンクになります）

#### 強化学習

強化学習に関しておすすめの本は、下記の本たちです。

ゼロから作るDeep Learning ❹ ―強化学習編
まずは、定番の本です。
強化学習に関して、本当に最初の部分（マルコフ決定過程とは何か）から、非常に詳細かつわかりやすく解説されており、深層学習を利用して、どのように強化学習をすることできるのかまで、しっかり書かれているためおすすめです。
また、コードと理論のバランスもちょうどよく、理系大学生とかであれば問題なく読み進められると思います。
特に、強化学習の基礎の部分が、詳細かつ分かりやすく書かれているため、一冊目におすすめです。

...(about 91 lines omitted)...
下の本に関しては、2024年の本ということもあり、近年の強化学習の実世界での応用と、理論的な定式化の架け橋になっているような本になっており、非常に面白いです。

#### AlphaGO Zero

AlphaZero 深層学習・強化学習・探索 人工知能プログラミング実践入門

だいぶ昔に購入させていただいた本ですが、まさか2025年に活躍することがあろうとは。
という感じですが、こちらの書籍も参考にさせていただきました。
Alpha Zeroを作る上で核となる技術を余すところなく解説されており、私は非常に勉強になりました。
Alpha Zeroに興味がある方は、ぜひ一度ご一読ください。

#### LLM、RLHF, DPO

大規模言語モデル入門Ⅱ〜生成型LLMの実装と評価
よく紹介させていただいておりますが、こちらの書籍は、LLMのファインチューニングから、RLHF、RAG、分散学習にかけて、本当に幅広く解説されており、いつも参考にさせていただいております。
今回のDPOやRLHFの解説部分でも、とても活躍してくれました。
LLMを取り扱っている方は、とりあえず買っておいても損はないと思います。
さまざまな章、ページで読めば読むほど新しい発見が得られる、スルメのような本だなといつも思っております。

