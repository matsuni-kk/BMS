# The AI CUDA Engineer: Agentic CUDA Kernel Discovery, Optimization and Composition

2025年2月20日

_**AI CUDAエンジニア**は、PyTorchにおける一般的な機械学習操作に対して10〜100倍の高速化を達成する、高度に最適化されたCUDAカーネルを生成できるエージェントシステムです。_

---

Sakana AIでは、より強力なAIシステムを開発するための道は、AIを使用してAIの開発を自動化することであると信じています。私たちは、さらに高性能で効率的なAIシステムを作成できるAIシステムを開発することを目指しています。

昨年、私たちは、AI基盤モデルの作成を自動化するAIシステムを、わずかなコストで導入しました。LLMがLLMをトレーニングするためのより効率的な方法を発明できることを示しました。最近、私たちはAI研究プロセス全体を完全に自動化するための最初の包括的なエージェントフレームワークをAI科学者で提案しました。これは私たちに疑問を抱かせました。AIをAI研究の実施に使用できる場合、AIをより高速に実行する方法を研究するためにAIを使用できるでしょうか？

## 導入

人間の脳と同様に、最新のAIシステムもGPUなどのハードウェアアクセラレータによって可能になる並列処理に大きく依存しています。しかし、資源制約下で効率的に動作するように進化（生物学的および文化的に）した人間の脳とは異なり、AI基盤モデルの最近の進歩は、大規模な展開と、ますます増大する推論時間とエネルギー需要につながり、AIモデルをトレーニングおよび展開するための資源要件が指数関数的に増加しています。

私たちは、根本的に、最新のAIシステムは人間の脳と同じくらい効率的であるべきであり、この効率を達成するための最良の道は、AIを使用してAIをより効率的にすることであると信じています！AI科学者に関する以前の研究に触発されて、**AI CUDAエンジニア**、完全に自動化されたCUDAカーネルの発見と最適化のための最初の包括的なエージェントフレームワークを発表できることを誇りに思います。

CUDAは、NVIDIA GPUの並列計算用のハードウェア命令セットへの直接アクセスを提供する低レベルのソフトウェア層です。CUDAカーネルは、GPU上で実行されるCUDA言語で記述された関数です。CUDAカーネルレベルで命令を直接記述することにより、AIアルゴリズムのはるかに高いパフォーマンスを達成できます。ただし、CUDAを操作するには、GPUに関するかなりの知識が必要であり、実際には、ほとんどの機械学習アルゴリズムは、PyTorchやJAXなどのより高いレベルの抽象化レイヤーで記述されています。

AI CUDAエンジニアは、標準のPyTorchコードを高度に最適化されたCUDAカーネルに_自動的に_変換することを目標に、最先端のLLMを活用するエージェントフレームワークです。進化的最適化の使用、および進化的計算の概念（_「クロスオーバー」_操作や有望な_「踏み石」_カーネルを発見するための_「イノベーションアーカイブ」_など）を活用することにより、提案されたフレームワークは、PyTorchモジュールからCUDAカーネルへの変換プロセスを自動化できるだけでなく、高度に最適化されたCUDAカーネルは、多くの場合、ランタイムが大幅に高速化される高速化を達成します。

このテクノロジーにより、LLMやその他の生成AIモデルなどの基盤モデルのトレーニングと実行（推論）の両方を加速する高速化が可能になり、最終的にはAIモデルがNVIDIAハードウェア上で大幅に高速に実行されるようになると信じています。

AI CUDAエンジニアは、一般的なPyTorch操作よりも**10〜100倍**高速なCUDAカーネルを生成できます。私たちのフレームワークは、すでに本番環境で一般的に使用されている既存のCUDAカーネルよりもはるかに高速な（最大**5倍**の高速化）高度に最適化されたCUDAカーネルを生成することもできます。

_AI CUDAエンジニアエージェントフレームワークの概要_

**ステージ1と2（変換と翻訳）：** AI CUDAエンジニアは、最初にPyTorchコードを機能するCUDAカーネルに変換します。明示的にターゲットを設定しなくても、初期ランタイムの改善がすでに観察されています。

**ステージ3（進化的最適化）：** 生物学的進化に触発されて、私たちのフレームワークは、進化的最適化（「適者生存」）を利用して、最高のCUDAカーネルのみが生成されるようにします。さらに、複数の最適化されたカーネルを補完的な方法で組み合わせるための、新しいカーネルクロスオーバープロンプト戦略を導入します。

**ステージ4（イノベーションアーカイブ）：** 文化的な進化が何千年にもわたる文明を通じて祖先からのノウハウで人間の知性を形作ったように、AI CUDAエンジニアは、過去のイノベーションと発見から学んだこと（ステージ4）も活用し、既知の高性能CUDAカーネルの祖先からイノベーションアーカイブを構築します。これは、さらなる翻訳とパフォーマンスの向上を達成するために、以前の踏み石を使用します。

## AI CUDAエンジニアによって発見されたカーネルランタイムの高速化

AI CUDAエンジニアは、一般的な機械学習操作に使用されるCUDAカーネルを堅牢に発見し、**PyTorchのネイティブおよびコンパイル済みカーネルよりも10〜100倍高速**な高速化を実現しました。私たちのアプローチは、機械学習アーキテクチャ全体を最適化されたCUDAカーネルに変換することもできます。ここでは、完全に自律的に行われたいくつかの重要な高速化の発見に焦点を当てます。

_これらの最適化されたCUDAカーネルの詳細については、インタラクティブWebサイトのリーダーボードをご覧ください。_

私たちのアプローチは、行列乗算などの基本的な操作から一般的な深層学習操作まで、より効率的なCUDAカーネルを見つけます。執筆時点では、発見されたCUDAカーネルのパフォーマンスはKernelBenchで最先端のパフォーマンスを達成しました。

## 技術レポートとデータセットのリリース

私たちは、これはAIの偉大な最適化の始まりに過ぎないと信じています！

新しい論文「**AI CUDAエンジニア：エージェントCUDAカーネルの発見と最適化**」をリリースできることを嬉しく思います。

**レポート**では：

* PyTorchコードを動作するCUDAカーネルに変換し、CUDAランタイムパフォーマンスを最適化し、複数のカーネルを自動的に融合できるエンドツーエンドのエージェントワークフローを紹介します。
* さらに、LLMアンサンブル、反復プロファイリングフィードバックループ、ローカルカーネルコード編集、クロスオーバーカーネル最適化など、パイプラインの一貫性とパフォーマンスを向上させるためのさまざまな手法を構築します。
* AI CUDAエンジニアは、検討された250のtorch操作のうち230以上を堅牢に変換し、ほとんどのカーネルで強力なランタイムパフォーマンスの向上を達成することを示します。さらに、私たちのアプローチは、さまざまなカーネル操作を効率的に融合することができ、既存のいくつかの高速化された操作よりも優れたパフォーマンスを発揮できます。
* 幅広いPyTorch操作をカバーする操作のために、17,000を超える検証済みカーネルの**データセット**をリリースします。

AIモデルの主要な計算操作で大幅な高速化を達成した、発見されたCUDAカーネルの注目すべき例をいくつか紹介します。

### 注目のAI CUDAエンジニアが発見したカーネル

私たちの新しいLLM駆動の進化的カーネル最適化手順を活用することで、多様な考慮事項に対して堅牢に高速化を実現します。より具体的には、検討された229のタスクのうち81％でPyTorchネイティブランタイムよりも優れたパフォーマンスを発揮します。さらに、発見されたすべてのCUDAカーネルの20％は、PyTorch実装よりも少なくとも2倍高速です。

_AI CUDAエンジニアは、PyTorch実装よりも優れたパフォーマンスを発揮するCUDAカーネルを堅牢に発見します。_

以下に、カーネルのサブセットを示します。これらは、AI CUDAエンジニアを正常に展開できるさまざまな操作の多様性を強調しています。これには、正規化手法、損失関数、特殊な行列乗算、さらにはニューラルネットワークアーキテクチャ全体が含まれます。

| [インスタンス正規化カーネル](https://pub.sakana.ai/ai-cuda-engineer/kernel/1/34/optimize-b5-s4-e1-sweep/3/3/1/34%5Finstnorm%5Fopt%5Fblocksize%5Fedit%5F1) | [効率的な三角行列乗算カーネル](https://pub.sakana.ai/ai-cuda-engineer/kernel/1/15/optimize-b5-s4-e1-sweep/3/2/1/strided%5Fefficient%5Ftriangular%5Fmm%5Fedit%5F1) | [モジュラークロスエントロピー損失カーネル](https://pub.sakana.ai/ai-cuda-engineer/kernel/1/95/optimize-b10-s4-e0-sweep/5/0/0/modular%5Fcrossentropy%5Fbase) | [最適化されたレイヤー正規化ストリームカーネル](https://pub.sakana.ai/ai-cuda-engineer/kernel/1/40/optimize-b5-s4-e1-v2/4/3/0/optimized%5Flayernorm%5Fstreamed%5Fbase) |
|---|---|---|---|
| [LeNet5融合カーネル](https://pub.sakana.ai/ai-cuda-engineer/kernel/3/4/optimize-b5-s4-e1-v2/5/2/0/4%5FLeNet5%5Ffused%5Feven%5Fbase) | [EfficientNet-B2ワープシャッフル最適化カーネル](https://pub.sakana.ai/ai-cuda-engineer/kernel/3/24/optimize-b10-s4-e0-sweep/10/3/0/efficientnetb2%5Fwarp%5Fshuffle%5Foptimization%5Fbase%5Fbase) | [GRU定数メモリカーネル](https://pub.sakana.ai/ai-cuda-engineer/kernel/3/39/optimize-b5-s4-e1-v2/5/1/1/39%5Fgru%5Fconstant%5Fmemory%5Fedit%5F1) | [ブロックサイズ調整コサイン損失カーネル](https://pub.sakana.ai/ai-cuda-engineer/kernel/1/97/optimize-b10-s4-e0-sweep/9/2/0/blocksize%5Ftuning%5Fcosine%5Floss%5Fbase) |

_**AI CUDAエンジニアによって生成された高度に最適化されたCUDAカーネルの例。**上記のCUDAカーネルの個々のサムネイルをクリックして、インタラクティブWebサイトでランタイムの高速化などの詳細と詳細な分析をご覧ください。_

## AI CUDAエンジニアアーカイブ：17,000以上の検証済みCUDAカーネルのデータセット

_AI CUDAエンジニアアーカイブのテキスト埋め込み視覚化は、発見されたカーネルがタスク（MatMul、プーリング、畳み込みなど）と実装戦略（アンローリング、融合、ベクトル化）にグループ化されていることを示しています。アーカイブはオープンにアクセス可能であり、LLMのダウンストリーム微調整に使用できます。_

この論文とともに、AI CUDAエンジニアによって生成された30,000を超えるCUDAカーネルで構成されるデータセットであるAI CUDAエンジニアアーカイブをリリースします。これはCC-By-4.0ライセンスでリリースされており、HuggingFace経由でアクセスできます。データセットには、torchリファレンス実装、torch、NCU、Clang-tidyプロファイリングデータ、タスクごとの複数のカーネル、エラーメッセージ、およびtorchネイティブおよびコンパイルランタイムに対する高速化スコアが含まれています。

_30,000を超えるカーネルと17,000を超える正しい検証済み実装で構成されるAI CUDAエンジニアアーカイブの要約統計。約50％のすべてのカーネルがtorchネイティブランタイムよりも改善されています。_

このデータセットにより、オープンソースモデルのポストトレーニングが可能になり、CUDA対応モジュールをより適切に実行できるようになると考えています。これには、オフライン強化学習、選好最適化、および標準的な教師あり微調整が含まれます。

## AI CUDAエンジニアアーカイブで17,000以上のカーネルを探索する

また、17,000を超える検証済みカーネルとそのプロファイル（torch、NCU、Clang-Tidyデータを含む）をインタラクティブに検査するためのインタラクティブWebサイトも公開しました。インタラクティブWebサイトには**こちら**からアクセスできます。

このWebサイトでは、230のタスクにわたるさまざまな高性能カーネルを探索できます。実験とLLMにまたがる関連カーネルを検査するために使用できるカスタムリーダーボードが付属しています。

_AI CUDAエンジニアによって発見されたカーネルのリーダーボード_

さらに、カーネルを視覚化し、関連カーネルを取得し、コードをダウンロードして実装と高速化を検証し、取得したプロファイリングデータを表示できます。最後に、最適化実験を詳細に調べることができます。

_プロファイリングデータ、評価スクリプトのダウンロード、関連カーネル、発見実験の詳細など、最適化されたインスタンス正規化カーネルの詳細ビュー。_

## AI CUDAエンジニアの将来的な影響

AI革命は始まったばかりであり、私たちは変革サイクルのまさに始まりにいます。今日のLLMは私たちの世代の「メインフレームコンピュータ」であるというのが私たちの見解です。私たちはまだAIの非常に初期段階にあり、市場競争とグローバルなイノベーション（特にリソース制約のあるイノベーション）により、このテクノロジーは100万倍効率的になることは避けられません。

現在、私たちのAIシステムは膨大なリソースを消費しており、テクノロジーが効率とエネルギー消費を考慮せずに拡大し続ける場合、その結果は持続可能ではありません。私たちのAIシステムが人間の知性と同じくらい効率的（またはそれ以上に効率的）であってはならない根本的な理由はありません。このより大きな効率を達成するための最良の道は、AIを使用してAIをより効率的にすることであると信じています。

これはSakana AIが追求している方向であり、このプロジェクトはAIを100万倍高速化するための重要なステップです。初期の扱いにくいメインフレームコンピュータから最新のコンピューティングへの進化と同様に、今日のAIの使用方法は、今日の「扱いにくい」、非効率なLLMと比較して、数年後には大きく異なって見えるでしょう。

## Sakana AI

AIを改善するAIを作成しませんか？詳細については、採用情報ページをご覧ください。

© Sakana AI株式会社
____ 